input {
    ...
}

filter {

    # Clone this event to create a new metric event with lag by time metrics
    mutate {
        add_field => { "[@metadata][enable_kafka_lag_event]" => "{{key_or_default "lma-settings/logstash/enable_lag_monitor" "false"}}"}
    }

    if [@metadata][enable_kafka_lag_event] == 'true' {
        clone {
            clones => ['kafka_lag_event']
        }
    } else {
        mutate {
            add_field => { "[@metadata][kafka_lag_event]" => "false" }
        }
    }

    if [type] == 'kafka_lag_event' {

        prune { whitelist_names => [ "datacenter", "kafka_append_time_producer", "kafka_topic_producer", "kafka_consumer_group_producer", "logstash_kafka_read_time_producer"] }

        # Add new field to track this in the output section; added here so prune{} doesn't clobber them
        mutate {
            add_field => { "[@metadata][kafka_lag_event]" => "true" }
            add_field => { "[@metadata][AGGREGATION_SINK_TOPIC_NAME]" => "metrics_lma" }
        }

        # Copy the kafka metadata into fields in our avro
        mutate {
            copy => {
                "[@metadata][kafka][timestamp]"      => "kafka_append_time_aggregate"
                "[@metadata][kafka][topic]"          => "kafka_topic_aggregate"
                "[@metadata][kafka][consumer_group]" => "kafka_consumer_group_aggregate"
            }
        }

        # Calculate the lag values we need and influx insert time for payload
        ruby {
            code => 'logstash_kafka_read_time_aggregate = (Time.now.to_f * 1000).to_i
                    event.set( "logstash_kafka_read_time_aggregate", logstash_kafka_read_time_aggregate )
                    kafka_logstash_influx_metric_time = (Time.now.to_f * (1000*1000*1000)).to_i
                    event.set( "kafka_logstash_influx_metric_time", kafka_logstash_influx_metric_time )
                    event.set( "kafka_aggregate_lag_ms", logstash_kafka_read_time_aggregate - event.get("kafka_append_time_aggregate") )'
        }

        # Calculate total lag if we have the values
        if [logstash_kafka_read_time_producer] and [kafka_append_time_producer] {
            ruby {
                code => 'event.set( "kafka_producer_lag_ms", event.get("logstash_kafka_read_time_producer") - event.get("kafka_append_time_producer") )
                        event.set( "kafka_total_lag_ms", event.get("logstash_kafka_read_time_aggregate") - event.get("kafka_append_time_producer") )'
            }

            mutate {
                add_field => {
                    "payload" => "kafka_lag_time,topic_aggregate=%{kafka_topic_aggregate},consumer_group_aggregate=%{kafka_consumer_group_aggregate},kafka_topic_producer=%{kafka_topic_producer},kafka_consumer_group_producer=%{kafka_consumer_group_producer} kafka_aggregate_lag_ms=%{kafka_aggregate_lag_ms},kafka_producer_lag_ms=%{kafka_producer_lag_ms},kafka_total_lag_ms=%{kafka_total_lag_ms} %{kafka_logstash_influx_metric_time}"
                }
            }
        } else {
            mutate {
                add_field => {
                    "payload" => "kafka_lag_time,topic_aggregate=%{kafka_topic_aggregate},consumer_group_aggregate=%{kafka_consumer_group_aggregate} kafka_aggregate_lag_ms=%{kafka_aggregate_lag_ms} %{kafka_logstash_influx_metric_time}"
                }
            }
        }

        # Add the fields needed to avro encode the message
        mutate {
            add_field => {
            "payload_type"   => "metric"
            "payload_format" => "line"
            "tags"           => "lma.write"
            "source"         => "lma"
            "servicelevel"   => "test"
            "type"           => "app"
            "loglevel"       => "none"
            }
        }

        # Prune out all our extra data.
        prune {
            blacklist_names => [
            "kafka_logstash_influx_metric_time",
            "kafka_aggregate_lag_ms",
            "kafka_producer_lag_ms",
            "kafka_total_lag_ms",
            "kafka_append_time_producer",
            "kafka_topic_producer",
            "kafka_consumer_producer",
            "logstash_kafka_read_time_producer",
            "kafka_append_time_aggregate",
            "kafka_topic_aggregate",
            "kafka_consumer_group_aggregate",
            "logstash_kafka_read_time_aggregate"
            ]
        }
    }
}

output {

    if [@metadata][kafka_lag_event] == 'true' {
        if [payload] and [payload] != "" {
            if ([@metadata][AGGREGATION_SINK_TOPIC_NAME]) {
                kafka {
                    topic_id                  => "%{[@metadata][AGGREGATION_SINK_TOPIC_NAME]}"
                    bootstrap_servers         => "lmabufaiad2-kf.intlma.wbx2.com:9093"
                    id                        => "metrics-lmabufottaint6-to-lmabufaiad2-shipper-kafka-out0"
                    compression_type          => "gzip"
                    batch_size                => "15000"
                    linger_ms                 => "100"
                    buffer_memory             => "67108864"

                    security_protocol         => "SSL"
                    ssl_truststore_location   => "/etc/logstash/ssl/client.truststore.jks"
                    ssl_truststore_password   => "WB6ZT5"
                    ssl_keystore_location     => "/etc/logstash/ssl/client.keystore.jks"
                    ssl_keystore_password     => "0IHQN7"


                    retries         => 5

                    key_serializer            => "org.apache.kafka.common.serialization.ByteArraySerializer"
                    value_serializer          => "org.apache.kafka.common.serialization.ByteArraySerializer"
                    codec => avro_schema_registry {
                        endpoint            => "https://prod-kafka-schema-registry.prodksr.wbx2.com:8082"
                        subject_name        => "LmaEventSchema"
                        schema_version      => 2
                        client_key          => "/etc/logstash/ssl/schema_registry_client.key"
                        client_certificate  => "/etc/logstash/ssl/schema_registry_client.crt"
                        ca_certificate      => "/etc/logstash/ssl/schema_registry_ca_cert.pem"
                        verify_mode         => "verify_peer"
                        binary_encoded      => false
                        base64_encoded      => false
                    }
                }
            }
        }
    } else {
        if  "diagnostics" in [tags] {
            elasticsearch {
            hosts              => ["role-data.logs6aiad2-es-app.service.consul:9200"]
            ilm_enabled        => true
            ilm_rollover_alias => "logstash-app-diagnostics-ilm"
            ilm_policy         => "logstash-app-diagnostics"
            ilm_pattern        => "{now/d}-000001"
            id                 => "indexer-lmabufaiad2-app-elasticsearch-out-diagnostics-alias"
            manage_template    => false


            # user/password
            user               => "logstash"
            password           => "YE7Ta4aHOaYaf0yU"
            sniffing         => true
            }
        } else if "metrics" in [tags] {
            elasticsearch {
            hosts              => ["role-data.logs6aiad2-es-app.service.consul:9200"]
            ilm_enabled        => true
            ilm_rollover_alias => "logstash-app-metrics-ilm"
            ilm_policy         => "logstash-app-metrics"
            ilm_pattern        => "{now/d}-000001"
            id                 => "indexer-lmabufaiad2-app-elasticsearch-out-metrics-alias"
            manage_template    => false


            # user/password
            user               => "logstash"
            password           => "YE7Ta4aHOaYaf0yU"
            sniffing         => true
            }
        } else {
            elasticsearch {
                hosts              => ["role-data.logs6aiad2-es-app.service.consul:9200"]
                index              => "%{[@metadata][AGGREGATION_SINK_INDEX]}"
                id                 => "indexer-lmabufaiad2-app-elasticsearch-out"
                manage_template    => false


                # user/password
                user               => "logstash"
                password           => "YE7Ta4aHOaYaf0yU"
                sniffing         => true
            }
        }
    }    
}