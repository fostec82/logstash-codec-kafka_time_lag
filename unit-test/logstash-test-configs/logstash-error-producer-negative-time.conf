input {
    stdin{}
}

filter {

  mutate {
    # add_field => { "[@metadata][metric_measurement_name]"                  => "chris_kafka_aiad2_lag" }
    #"[@metadata][kafka_topic_producer]"              => "kafka_topic_producer"
    # "[@metadata][kafka_consumer_group_aggregate]"    => "kafka_consumer_group_aggregate"
    add_field => {
      "[@metadata][kafka_datacenter_producer]"         => "kafka_datacenter_producer"
      "[@metadata][kafka_topic_producer]"              => "kafka_topic_producer"
      "[@metadata][kafka_consumer_group_producer]"     => "kafka_consumer_group_producer"
      "[@metadata][kafka_append_time_producer]"        => -1
      "[@metadata][logstash_kafka_read_time_producer]" => 200
      "[@metadata][kafka_datacenter_aggregate]"        => "kafka_datacenter_aggregate"
      "[@metadata][kafka_topic_aggregate]"             => "kafka_topic_aggregate"
      "[@metadata][kafka_consumer_group_aggregate]"    => "kafka_consumer_group_aggregate"
      "[@metadata][kafka_append_time_aggregate]"       => 300
    }
  }

  ruby {
    code => 'logstash_kafka_read_time_aggregate = (Time.now.to_f * 1000).to_i
             event.set( "[@metadata][logstash_kafka_read_time_aggregate]", logstash_kafka_read_time_aggregate )'
  }

  # mutate {
  #   remove_field => [ "message" ]
  # }

  # mutate {
  #   remove_field => [ "host" ]
  # }

}
output {

  # stdout { codec => rubydebug { metadata => true }}
  # stdout { codec => plain }
  stdout{
     codec => kafkatimemachine {}
   }
}